---
title: '**Research Log**'
author: "Rick Venema"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("pander")
library(ggplot2)
library(gplots)
library(dendextend)
library(RWeka)
library(mldr)
```

\newpage

# General introduction
In this research log, research is described that was carried out at the Hanze University of Applied sciences. This research is an attempt to create a classifier based on a forest data set. This dataset was used by Johnson et al. (2012) \cite{Johnson12}, their goal was to map different types of forests by using spectral data. The data set is available via the link https://archive.ics.uci.edu/ml/datasets/Forest+type+mapping \cite{Dataset}.

## License
```{}
Research log of thema 9
Copyright (C) 2018  Rick Venema

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
```

## About the research
In the study by Johnson et al. (2012)\cite{Johnson12}, geographically weighted variables calculated for two tree species were used in addition to spectral information to classify the two different tree types in a mixed forest. Numeric spectral values were used as trainings samples for predicting spectral values at other locations using the inverse distance weighting (IDW) interpolation method. Next, the similarity between spectral values and their IDW predicted values was calculated for both tree species. This simularity is considered geographically weighted because nearer training samples have more of an impact on their calculations. This resulted in an increase in overall accuracy.

# Research question
Based on the research chosen, a research question was created to further improve a model that can classify different forest types in a mixed forest. The question that was eventually created needed to include ML algorithms. The first question that was created was validated by classmates. 
```{}
Can a Machine learning algorithm give accurate results in recognizing different tree
types in a forest.
```

This question had different comments, that were taken into account in improving the research question.

```{}
What difference can a machine learning algorithm produce when recognizing different 
tree types in a forest, taking geographical weighted variables into account?
```

\newpage
# Dataset

## Dataset description
This description is the description included with the site at which the data can be found, it gives a clear description on what kind of data we are dealing with.
  
`This data set contains training and testing data from a remote sensing study which mapped different forest types based on their spectral characteristics at visible-to-near infrared wavelengths, using ASTER satellite imagery. The output (forest type map) can be used to identify and/or quantify the ecosystem services (e.g. carbon storage, erosion protection) provided by the forest.`
  
## Layout of the data
```{r loadingData, echo=F}
rawDataTesting <- read.csv("../data/ForestTypes/testing.csv")
rawDataTraining <- read.csv("../data/ForestTypes/training.csv")
```
The dimensions of rawDataTesting is equal to `r pander(dim(rawDataTesting))`. The first value represents the amount of rows and the second value represents the number of columns. The rows represent the instances, while the columns represent the attributes. The dimensions of rawDataTraining is equal to `r pander(dim(rawDataTraining))`.

### The class attribute
The data has a class attribute, this class attribute can have different values which are presented in table \ref{ClassAttributes}


```{r ClassAttributes, echo=F}
Sugi <-  c("s", "'Sugi' forest")
Hinoki <-  c("h", "'Hinoki' forest")
Mixed <- c("d", "'Mixed deciduous' forest")
Other <- c("o", "'Other' non-forest land")

colnamesClass <- c("Abbrevation", "Full name")
ClassAttributes <- as.data.frame(rbind(Sugi, Hinoki, Mixed, Other), row.names = 1)
colnames(ClassAttributes) <- colnamesClass
set.caption("\\label{ClassAttributes}Class Attributes")
pander(pandoc.table(ClassAttributes))
```

### The b columns
In figure \ref{bColumnsUnsorted} the log2 transformed data of the b columns. The boxplot give a difference in the different years, this is because the different columns can be divided into 
3 different years. The first 3 boxplots represent the data from 26 September 2010, the next three boxplots represent the data from 19 March 2011, and the last three boxplots represent the data from 8 May 2011. The different colors
of the boxplots represent the different spectral wavelengths. The green color represent their corresponding wavelength green(0.52-0.60 ?m), red (0.63-0.69 ?m) and near-infrared (NIR) (0.76-0.86 ?m). NIR is represented as violet in the boxplots.


```{r bColumnsUnsortedTXT, echo=F, eval=F}
bColumns <- rawDataTraining[2:10]
boxplot(log2(bColumns), col=c("green", "red", "violet"), 
        xlab="The different b columns", ylab="Log2 transformed values", 
        main="Boxplot showing the log2 transformed data of the b columns")
```

### The b columns orderd by class
To get a better view of the differences between dates, the data was split into the 4 different class attributes. Each different kind of forest was put into a boxplot, which can be seen in figure \ref{bColumnsSorted}. As can be seen, the dates differ a lot. This can be due to the different seasons at which the pictures were taken. The boxplots created from the March 2011 data, is higher than the other dates. THe data needs to be divided into the 3 groups. Each data has 3 boxplots, which each corresponding to the different spectral bands that are used in the research. These bands differ quite a bit, just like the boxplots in figure \ref{bColumnsUnsorted}. This can be due to the different seasons at which the images were taken. Different seasons have different values for the different spectral bands. The spectral bands are collected by ASTRAL imagery.
```{r bColumnsSortedTXT, echo=F, fig.height=6, out.extra = '', eval=F}
bHinokiRows <- bColumns[rawDataTraining$class=='h ',]
bSugiRows <- bColumns[rawDataTraining$class=='s ',]
bMixedRows <- bColumns[rawDataTraining$class=='d ',]
bOtherRows <- bColumns[rawDataTraining$class=='o ',]

# Creates Boxplots of the bcolumns
par(mfrow=c(2,2))
boxplot(bHinokiRows, main="The b columns with class 'Hinoki' (a)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bSugiRows, main="The b columns with class 'Sugi' (b)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bMixedRows, main="The b columns with class 'Mixed' (c)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bOtherRows, main="The b columns with class 'Other' (d)"
        , xlab="The different b columns", ylab="Log2 transformed values")
```

### Plots

#### Figure \ref{bColumnsUnsorted}
```{r bColumnsUnsorted, echo=F, fig.cap="\\label{bColumnsUnsorted} The log2 transformed values of the b columns.", fig.pos="!h", out.extra=""}
source("../scripts/BoxplotsbColumns.R")
```

\newpage
#### Figure \ref{bColumnsSorted}
```{r bColumnsSorted, echo=F, fig.height=6, fig.cap="\\label{bColumnsSorted}B columns divided by class attribute", fig.pos="!h", out.extra=""}
bColumns <- rawDataTraining[2:10]
bHinokiRows <- log(bColumns[rawDataTraining$class=='h ',])
bSugiRows <- log(bColumns[rawDataTraining$class=='s ',])
bMixedRows <- log(bColumns[rawDataTraining$class=='d ',])
bOtherRows <- log(bColumns[rawDataTraining$class=='o ',])

par(mfrow=c(2,2))
boxplot(bHinokiRows, main="The b columns with class 'Hinoki' (a)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bSugiRows, main="The b columns with class 'Sugi' (b)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bMixedRows, main="The b columns with class 'Mixed' (c)"
        , xlab="The different b columns", ylab="Log2 transformed values")
boxplot(bOtherRows, main="The b columns with class 'Other' (d)"
        , xlab="The different b columns", ylab="Log2 transformed values")
```


### pred_minus_obs columns
The `pred_minus_obs` columns represent the Predicted spectral values minus actual spectral values for the `s` or `h` class. These values are calculated based on spatial interpolation. For this calculation the IDW method was used, this method considers the values of nearby samples to predict the value at a given location. The added 18 columns are used to determine if geographically weighted variables influences the accuracy.


# EDA
## Evenly or unevenly classes
The classes are unevenly represented. The dimensions are printed next, ordered by class type

```{r evenuneven, echo=F}
# Splits the data based on classes
bColumns <- rawDataTraining[2:10]
bHinokiRows <- bColumns[rawDataTraining$class=='h ',]
bSugiRows <- bColumns[rawDataTraining$class=='s ',]
bMixedRows <- bColumns[rawDataTraining$class=='d ',]
bOtherRows <- bColumns[rawDataTraining$class=='o ',]

# Prints the amounts of classified classes
pander("Hinoki amount: ") 
pander(dim(bHinokiRows)[1])
pander(", Sugi amount: ")
pander(dim(bSugiRows)[1])
pander(", Mixed amount: ")
pander(dim(bMixedRows)[1])
pander(", Other amount: ")
pander(dim(bOtherRows)[1])
```

## Missing data
To check if there is missing values the dataset needs to be searched for NA values. 
  
```{r NAcheck, echo=F}
# code to check if there are missing values
pander("Is any number NA? ")
pander(any(is.na(rawDataTraining)))
```

This check has been done by using the `any()` function and the `is.na()` function.

## Correlation in the data
The data has some correlation, the different columns respresent the different types of wavelength per date. The columns can be summed per 3 columns, the b1-b9 represent 3 dates evenly distributed. Meaning that 9 columns are a representation of 3 different dates. The next 9 columns are IDW interpolated dates in the same distribution of the first 9 b columns.

## Hierarchical clustering
A hierarchical clustering was performed to get a view of the data clustered into the different groups. Each group is clustered into different groups. As figure \ref{histColor} shows, the data is clustered mostly in the correct groups, however some of the data is not clustered right. This can have different reasons, especially when the Mixed class and Other class are not correctly clustered.
```{r histColorTXT, echo=F, eval=F}
# Create the dend:
dend <- as.dendrogram(hclust(dist(rawDataTraining)))

# Create a vector giving a color for each car to which company it belongs to
Forest_type <- rep("Other", length(rownames(rawDataTraining)))
is_x <- grepl("h ", rawDataTraining$class)
Forest_type[is_x] <- "Hinoki"
is_x <- grepl("s ", rawDataTraining$class)
Forest_type[is_x] <- "Sugi"
is_x <- grepl("d ", rawDataTraining$class)
Forest_type[is_x] <- "Mixed"

# settings for plot
Forest_type <- factor(Forest_type)
nForestTypes <- length(unique(Forest_type))
cols_4 <- colorspace::rainbow_hcl(nForestTypes, c = 70, l = 50)
colForestType <- cols_4[Forest_type]
k234 <- cutree(dend, k = 2:4)
labels_colors(dend) <- colForestType[order.dendrogram(dend)]

par(mar = c(4,1,1,12))
plot(dend, horiz = TRUE)
colored_bars(cbind(k234[,3:1], colForestType), dend, 
             rowLabels = c(paste0("k = ", 4:2), "Forest Type"), horiz = TRUE)
legend("topleft", legend = levels(Forest_type), fill = cols_4)
```

\newpage
## Plots
### Figure \ref{histColor}
```{r histColor, fig.cap="\\label{histColor}Hierarchical clustered data of the training Data", echo=F, warning=F, fig.height=7.5, fig.pos="!h", out.extra=''}
# Create the dend:
dend <- as.dendrogram(hclust(dist(rawDataTraining)))

# Create a vector giving a color for each car to which company it belongs to
Forest_type <- rep("Other", length(rownames(rawDataTraining)))
is_x <- grepl("h ", rawDataTraining$class)
Forest_type[is_x] <- "Hinoki"
is_x <- grepl("s ", rawDataTraining$class)
Forest_type[is_x] <- "Sugi"
is_x <- grepl("d ", rawDataTraining$class)
Forest_type[is_x] <- "Mixed"

Forest_type <- factor(Forest_type)
nForestTypes <- length(unique(Forest_type))
cols_4 <- colorspace::rainbow_hcl(nForestTypes, c = 70, l = 50)
colForestType <- cols_4[Forest_type]
k234 <- cutree(dend, k = 2:4)
labels_colors(dend) <- colForestType[order.dendrogram(dend)]

par(mar = c(4,1,1,12))
plot(dend, horiz = TRUE, main="Hierarchical clustering of the trainingsset ")
colored_bars(cbind(k234[,3:1], colForestType), dend, rowLabels = c(paste0("k = ", 4:2), "Forest Type"), horiz = TRUE)
legend("topleft", legend = levels(Forest_type), fill = cols_4)
```


## PCA
A Principal Component Analysis (PCA) was performed on the data. PCA is a dimensionality reduction technique that is used in data analysis. Reducing the dimensionality of a dataset can be useful in different ways\cite{PCA}. First a summary of the data was made. The three different rows per date were summed to get one column per date. This reduced the spread of the PCA. 
```{r PCATXT1, echo=F}
PCA.data <- rawDataTraining[2:10]

PCA.data <- data.frame(rowSums( PCA.data[,1:3 ]),
                        rowSums( PCA.data[,4:6 ]),
                        rowSums( PCA.data[,7:9])
                        )

colnames(PCA.data) <- c("26 September 2010", "19 March 2011", "8 May 2011")



PCA.data.log <- log(PCA.data)

PCA.bColumns <- prcomp(PCA.data.log,
                       center=TRUE,
                       scale. = TRUE)

# plot(PCA.bColumns, type="l")
set.caption("\\label{sumPCA}Summary of the PCA data")
pander(summary(PCA.data))
```

Next a Prediction was executed and the results are shown in \ref{predPCA}.

```{r PCATXT2, echo=F}

set.caption("\\label{predPCA}Prediction of the log2 data")
pander(predict(PCA.bColumns, 
        newdata=tail(PCA.data.log, 2)))



theta <- seq(0,2*pi,length.out = 100)
circle <- data.frame(x = cos(theta), y = sin(theta))
# p <- ggplot(circle,aes(x,y)) + geom_path()
# 
# loadings <- data.frame(PCA.bColumns$rotation, 
#                        .names = row.names(PCA.bColumns$rotation))
# p + geom_text(data=loadings, 
#               mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +
#   coord_fixed(ratio=1) +
# labs(x = "PC1", y = "PC2")
```

\newpage

## Plots
### Figure \ref{PCAa}
```{r PCAa, fig.cap="\\label{PCAa}", out.extra='', echo=F, fig.pos="!h"}
PCA.data <- rawDataTraining[2:10]

PCA.data <- data.frame(rowSums( PCA.data[,1:3 ]),
                        rowSums( PCA.data[,4:6 ]),
                        rowSums( PCA.data[,7:9])
                        )

colnames(PCA.data) <- c("26 September 2010", "19 March 2011", "8 May 2011")



PCA.data.log <- log(PCA.data)

PCA.bColumns <- prcomp(PCA.data.log,
                       center=TRUE,
                       scale. = TRUE)

plot(PCA.bColumns, type="l", main="variances associated with the PCs")
```

\newpage
### Figure \ref{PCAb}
```{r PCAb, echo=F, fig.cap="\\label{PCAb}", out.extra='', fig.pos="!h"}
# predict(PCA.bColumns, 
#         newdata=tail(PCA.data.log, 2))



theta <- seq(0,2*pi,length.out = 100)
circle <- data.frame(x = cos(theta), y = sin(theta))
p <- ggplot(circle,aes(x,y)) + geom_path()

loadings <- data.frame(PCA.bColumns$rotation, 
                       .names = row.names(PCA.bColumns$rotation))
p + geom_text(data=loadings, 
              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +
  coord_fixed(ratio=1) +
labs(x = "PC1", y = "PC2")
```

# Creating datasets
## In the research
In the research the complete data set was used for training, the non interpolated data was used for reference to determine if geographical weighted values have an influence in the accuracy in classification.

## Own Dataset
To create an own dataset, a couple different things need to be taken into account. For example; how do I need to split my data to create a dataset that has the geographical weighted variables and a dataset that only has the spectral values of the image. This research checks the difference in geographically weighted variables and normal spectral data, compared to onl spectral data. Meaning that 2 datasets needs to be created: the total amount of data, and only the spectral data.

```{r creatingDatasets, eval=F}
bColumnsToFile <- data.frame(rawDataTraining[2:10], rawDataTraining$class)
allDataToFile <- data.frame(rawDataTraining[2:28], rawDataTraining$class)

bColumnsToFileTesting <- data.frame(rawDataTesting[2:10], rawDataTesting$class)
allDataToFileTesting <- data.frame(rawDataTesting[2:28], rawDataTesting$class)

head(bColumnsToFile)
write.arff(bColumnsToFile, file="../data/bColumnsOnlyData.arff")
write.arff(allDataToFile, file="../data/CompleteTrainingsData.arff")

write.arff(bColumnsToFileTesting, file="../data/bColumnsOnlyDataTesting.arff")
write.arff(allDataToFileTesting, file="../data/CompleteTrainingsDataTesting.arff")
```

## Quality metrics 
Determining of quality metrics for the algorithm is an important part of assessing the ML algorithm. Each application in the real world has different requirements. To determine the quality metrics of a ML algorithm, the most important requirements need to be selected. In my case, the most important metric of my ML algorithm is speed. It is quite important that the algorithm can determine forest types on the fly. When a drone/airplane is flying over a forest, the algorthm needs to determine quickly what type of forest it is. Accuracy is not the most important part of the algorithm, it doesn't need to classify each part of the forest exactly right. 

# ML algorithm performance testing

## Algortihms used
The algorithm used in the original paper was a **SVM** with a **SVM-RBF** classifier. To further understand the **SVM** algorithms, Mountrakis et al.\cite{Mountrakis} was read. But to check if the chosen algorithm was the best, different algorithms were used to check the choice. For this validation, Jose et. al. \cite{jose} was used. This paper uses 4 different algorithms to test speed and accuracy. These four algorithms are **MLP**, **LG**, **SVM**, and **C4.5** tree. These are the most used algorithms in remote sensing studies. First a significant amount of algorithms were used to give a general idea of the accuracy of a lot of different algorithms. These algorithms are: **ZeroR**, **OneR**, **SimpleLogistic**, **IBk**, **NaiveBayes**, **C4.5 (J48)**, **SVM (SMO)**, and **RandomForest**. To extend the standard algorithms, **LG**, **MLP** were added. To test these different algorithms, the Weka experimenter was used. This Experimenter can handle multiple files and multiple algorithms, and these results can be further processed with different tests implemented into the Weka experimenter. These test were then read into R and processed on accuracy and speed. A confusion matrix was also made with the average TP, FP, TN, and FN. 

```{r Algorithms, echo=F}
algorithmsTestbColumns <- read.csv("../data/SVM-results-researchsettings-bColumns.csv")
algorithmsTestAll <- read.csv("../data/SVM-results-researchsettings-all.csv")

pander("Only spectral values dataset accuracy: ")
pander(sum(algorithmsTestbColumns$Percent_correct)/(dim(algorithmsTestbColumns)[1]))
pander(". Complete dataset accuracy: ")
pander(sum(algorithmsTestAll$Percent_correct)/(dim(algorithmsTestAll)[1]))
pander(".")

```

This means that the accuracy increases when the interpolated values are used. This is a confirmation to the original study. The question I want to answer, is, if this is the best algorithm to use. For this, different algorithms needed to be tested. To make the data more specific, only the complete dataset was used. Meaning that all 27 columns were used to evaluate the algorithm. Based on these results, more can be said about the used algorithm.
  
First, the standard algorithmes are evaluated, these can give a base line, and if there is a standard algorithm that outperforms the rest, that algorithm needs to be examined further.


```{r readingAlorithms, echo=F}

set.caption("\\label{Correct}Percentage Correct per algorithm")
ZeroR.results <- read.csv("../data/ZeroR-results-all.csv")
OneR.results <- read.csv("../data/OneR-results-all.csv")
IBk.results <- read.csv("../data/IBk-results-all.csv")
C45.results <- read.csv("../data/C45-results-all.csv")
LR.results <- read.csv("../data/LR-results-all.csv")
MPL.results <- read.csv("../data/MLP-results-all.csv")
RandomForest.results <- read.csv("../data/RandomForest-results-all.csv")
SL.results <- read.csv("../data/SL-results-all.csv")
SVM.results <- read.csv("../data/SVM-results-researchsettings-all.csv")

ZeroR.averageCorrect <- sum(ZeroR.results$Percent_correct)/(dim(ZeroR.results)[1])
OneR.averageCorrect <- sum(OneR.results$Percent_correct)/(dim(OneR.results)[1])
IBk.averageCorrect <- sum(IBk.results$Percent_correct)/(dim(IBk.results)[1])
C45.averageCorrect <- sum(C45.results$Percent_correct)/(dim(C45.results)[1])
LR.averageCorrect <- sum(LR.results$Percent_correct)/(dim(LR.results)[1])
MPL.averageCorrect <- sum(MPL.results$Percent_correct)/(dim(MPL.results)[1])
RandomForest.averageCorrect <- sum(RandomForest.results$Percent_correct)/(dim(RandomForest.results)[1])
SL.averageCorrect <- sum(SL.results$Percent_correct)/(dim(SL.results)[1])
SVM.averageCorrect <- sum(SVM.results$Percent_correct)/(dim(SVM.results)[1])

averageCorrect.dataframe <- data.frame(c("ZeroR", "OneR", "IBk", "C4.5", "Logistic Regression", "MultiLayerPerceptron", "RandomForest", "Simple Logistics", "Support Vector Machines"),
                                       c(ZeroR.averageCorrect, OneR.averageCorrect, IBk.averageCorrect, C45.averageCorrect, LR.averageCorrect, MPL.averageCorrect, RandomForest.averageCorrect, SL.averageCorrect, SVM.averageCorrect))

colnames(averageCorrect.dataframe) <- c("Algorithm", "Average Percentage Correct")
pander( averageCorrect.dataframe)
```

```{r TrainingTime, echo=F}

set.caption("\\label{TrainingTime}Time used for each algorithm to train the algorithm")
ZeroR.TimeTraining <- sum(ZeroR.results$Elapsed_Time_training)/(dim(ZeroR.results)[1])
OneR.TimeTraining <- sum(OneR.results$Elapsed_Time_training)/(dim(OneR.results)[1])
IBk.TimeTraining <- sum(IBk.results$Elapsed_Time_training)/(dim(IBk.results)[1])
C45.TimeTraining <- sum(C45.results$Elapsed_Time_training)/(dim(C45.results)[1])
LR.TimeTraining <- sum(LR.results$Elapsed_Time_training)/(dim(LR.results)[1])
MPL.TimeTraining <- sum(MPL.results$Elapsed_Time_training)/(dim(MPL.results)[1])
RandomForest.TimeTraining <- sum(RandomForest.results$Elapsed_Time_training)/(dim(RandomForest.results)[1])
SL.TimeTraining <- sum(SL.results$Elapsed_Time_training)/(dim(SL.results)[1])
SVM.TimeTraining <- sum(SVM.results$Elapsed_Time_training)/(dim(SVM.results)[1])

averageTimeTraining <- data.frame(c("ZeroR", "OneR", "IBk", "C4.5", "Logistic Regression", "MultiLayerPerceptron", "RandomForest", "Simple Logistics", "Support Vector Machines"),
                                  c(ZeroR.TimeTraining, OneR.TimeTraining, IBk.TimeTraining, C45.TimeTraining, LR.TimeTraining, MPL.TimeTraining, RandomForest.TimeTraining, SL.TimeTraining, SVM.TimeTraining))

colnames(averageTimeTraining) <- c("Algorithm", "Average Time Cost")
pander(averageTimeTraining)
```

The results from table \ref{TrainingTime} are gathered on a Intel Core i7-7700HQ and 16GB of RAM. In this table, the difference in elapsed time for training can be seen. The difference is quite clear, some algorithm take much longer. Some algorithms that take a long time to train, are also quite accurate.

```{r TestingTime, echo=F}
set.caption("\\label{TestingTime}Time used for each algorithm to test the accuracy")
ZeroR.TimeTrainingTesting <- sum(ZeroR.results$Elapsed_Time_testing)/(dim(ZeroR.results)[1])
OneR.TimeTrainingTesting <- sum(OneR.results$Elapsed_Time_testing)/(dim(OneR.results)[1])
IBk.TimeTrainingTesting <- sum(IBk.results$Elapsed_Time_testing)/(dim(IBk.results)[1])
C45.TimeTrainingTesting <- sum(C45.results$Elapsed_Time_testing)/(dim(C45.results)[1])
LR.TimeTrainingTesting <- sum(LR.results$Elapsed_Time_testing)/(dim(LR.results)[1])
MPL.TimeTrainingTesting <- sum(MPL.results$Elapsed_Time_testing)/(dim(MPL.results)[1])
RandomForest.TimeTrainingTesting <- sum(RandomForest.results$Elapsed_Time_testing)/(dim(RandomForest.results)[1])
SL.TimeTrainingTesting <- sum(SL.results$Elapsed_Time_testing)/(dim(SL.results)[1])
SVM.TimeTrainingTesting <- sum(SVM.results$Elapsed_Time_testing)/(dim(SVM.results)[1])

averageTimeTrainingTesting <- data.frame(c("ZeroR", "OneR", "IBk", "C4.5", "Logistic Regression", "MultiLayerPerceptron", "RandomForest", "Simple Logistics", "Support Vector Machines"),
                                  c(ZeroR.TimeTrainingTesting, OneR.TimeTrainingTesting, IBk.TimeTrainingTesting, C45.TimeTrainingTesting, LR.TimeTrainingTesting, 
                                    MPL.TimeTrainingTesting, RandomForest.TimeTrainingTesting, SL.TimeTrainingTesting, SVM.TimeTrainingTesting))

colnames(averageTimeTrainingTesting) <- c("Algorithm", "Average Time Cost")
pander(averageTimeTrainingTesting)
```

After looking into the different algorithms, no significant difference was found between SVM and MLP. To determine if I wanted to continue with MLP or with SVM, more research needed to be done. First I've read an article by Raczko and Zagajewski \cite{Raczko}. This article looks into the difference in SVM and MLP. They found out, that MLP performs better with less spectral bands. To find out, if the same results are true for the dataset that I use, I trained a model on only the spectral data.

```{r MLPvMLP}
MLP.All <- read.csv("../data/MLP-results-all.csv")
MLP.b <- read.csv("../data/MLP-results-bColumns.csv")

MLP.All.correct <- sum(MLP.All$Percent_correct)/(dim(MLP.All)[1])
MLP.b.correct <- sum(MLP.b$Percent_correct)/(dim(MLP.b)[1])

pander(MLP.All.correct)
pander(MLP.b.correct)
```

# Weka wrapper and missing items

## Weka wrapper
after a lot of work, a complete weka wrapper was written in java which can run the trained model via a command line application

## Roc curve
the first roc curve that was made, was not quite readable

```{r newRoc, echo=FALSE}
setwd("../data")
rocData <- read.arff("D:/School/thema 9/Thema9/RickVenemaProject/data/roc_SVM_MLP.arff")

plot(rocData$`True Positives` ~ rocData$`False Positives`, type="l", xlab="False Positives", ylab="True Positives")
```


\begin{thebibliography}{9}

\bibitem{Johnson12}
Johnson, B., Tateishi, R., Xie, Z., 2012. \textit{Using geographically-weighted variables for image classification. Remote Sensing Letters}, 3 (6), 491-499.

\bibitem{Dataset}
\textit{Forest type mapping Data Set} Retrieved September 11, 2018, from https://archive.ics.uci.edu/ml/datasets/Forest+type+mapping

\bibitem{PCA}
\textit{ Introduction to Principal Component Analysis (PCA)} Viewed September 24 2018, from https://tgmstat.wordpress.com/2013/11/21/introduction-to-principal-component-analysis-pca/

\bibitem{Russel88}
Russell, G., Congaltol., 1988 \textit{A Comparison of Sampling Schemes Used in Generating Error Matrices for Assessing the Accuracy of Maps Generated from Remotely Sensed Data }

\bibitem{Mountrakis}
Mountrakis, G.,Im, J., Ogole, C., 2011 \textit{A Support vector machines in remote sensing: A review }

\bibitem{jose}
Jose M. Pena, Pedro A. Gutierrez, Cesar Hervás-Martínez, Johan Six, Richard E. Plant and Francisca Lopez-Granados,  2014 \textit{Based Image Classification of Summer Crops with Machine Learning Methods }

\bibitem{Raczko}
Raczko, E., Zagajewski, B. 2017 \textit{Comparison of support vector machine, random forest and neural network classifiers for tree species classification on airborne hyperspectral APEX images}

\end{thebibliography}


